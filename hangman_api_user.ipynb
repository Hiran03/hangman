{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "To tackle the Hangman word prediction task, a **Transformer Encoder-Decoder** setup was deployed.\n",
    "\n",
    "\n",
    "##  Dataset\n",
    "\n",
    "Using a custom Python script, all possible hangman states for each word were generated. This yielded approximately **1 billion (xi, yi) pairs**, where:\n",
    "\n",
    "* **xi**: Masked hangman state of the word\n",
    "* **yi**: True word\n",
    "\n",
    "**Final dataset size: \\~3 GB**\n",
    "\n",
    "---\n",
    "\n",
    "##  Data Engineering\n",
    "\n",
    "* Each `(xi, yi)` pair is converted into a **PyTorch tensor** of shape `(seq_len, 27)`\n",
    "* Characters are **one-hot encoded**:\n",
    "\n",
    "  * `'a'` → index 0\n",
    "  * `'_'` (masked character) → index 26\n",
    "* Data is loaded using a **PyTorch DataLoader** with:\n",
    "\n",
    "  * `batch_size = 512`\n",
    "  * Padding per batch to match sequence length using zero vectors\n",
    "\n",
    "---\n",
    "\n",
    "##  Model Architecture\n",
    "\n",
    "A standard **Transformer Encoder-Decoder** was used with the following parameters:\n",
    "\n",
    "* `input_size = 27`\n",
    "* `d_model = 256`\n",
    "* `nhead = 8`\n",
    "* `num_layers = 4`\n",
    "* **Sinusoidal Positional Encoding**\n",
    "\n",
    "###  Training Details\n",
    "\n",
    "* **soft cross-entropy loss** for backpropagation\n",
    "* **Optimizer**: Adam (`lr = 1e-3`)\n",
    "* **Total parameters**: \\~2M (\\~10 MB)\n",
    "* **Epochs**: 1 (due to computational constraints)\n",
    "\n",
    "**Only training for 1 epoch limited model accuracy**\n",
    "\n",
    "---\n",
    "\n",
    "##  Results & Conclusion\n",
    "\n",
    "* Final Accuracy on practice runs (100 runs): **0.62**\n",
    "* Final Accuracy on recorded runs (338 runs): **0.553** [exhausted 662 on old models]\n",
    "* Baseline model accuracy: **0.18**\n",
    "* Challenge cutoff: **0.50** \n",
    "\n",
    "The model surpassed the baseline and cutoff even after **just 1 epoch** of training. Since transformers typically benefit from longer training or pretrained weights, performance is expected to improve with extended training.\n",
    "\n",
    "**Many successful transformer-based models are fine-tuned from large pretrained models.**\n",
    "\n",
    "---\n",
    "\n",
    "##  Python Files\n",
    "\n",
    "Attached at end of the notebook\n",
    "\n",
    "| File                   | Description                                                                                  |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| `create_data.py`       | Generates all possible hangman states from `words_250000_train.txt`                          |\n",
    "| `load_data.py`         | Builds a PyTorch DataLoader from the generated text file                                     |\n",
    "| `transformer_model.py` | Trains the transformer model and saves it to `.pth` format                                   |\n",
    "| `predict.py`           | Takes a masked hangman word as input and returns character guesses in descending probability |\n",
    "\n",
    "---\n",
    "\n",
    "##  Declaration\n",
    "\n",
    "All logic and code were written by me. I used **LLMs for debugging** and referencing library syntax when needed.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from predict import make_predictions\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        word_fixed = ''\n",
    "        for i in range(0, len(word), 2) :\n",
    "            word_fixed += word[i]\n",
    "        predictions = make_predictions(word_fixed)\n",
    "        for letter in predictions:\n",
    "            if (letter not in self.guessed_letters) and (letter not in word_fixed):\n",
    "                return letter\n",
    "        # ###############################################\n",
    "        # # Replace with your own \"guess\" function here #\n",
    "        # ###############################################\n",
    "\n",
    "        # # clean the word so that we strip away the space characters\n",
    "        # # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
    "        # clean_word = word[::2].replace(\"_\",\".\")\n",
    "        \n",
    "        # # find length of passed word\n",
    "        # len_word = len(clean_word)\n",
    "        \n",
    "        # # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
    "        # current_dictionary = self.current_dictionary\n",
    "        # new_dictionary = []\n",
    "        \n",
    "        # # iterate through all of the words in the old plausible dictionary\n",
    "        # for dict_word in current_dictionary:\n",
    "        #     # continue if the word is not of the appropriate length\n",
    "        #     if len(dict_word) != len_word:\n",
    "        #         continue\n",
    "                \n",
    "        #     # if dictionary word is a possible match then add it to the current dictionary\n",
    "        #     if re.match(clean_word,dict_word):\n",
    "        #         new_dictionary.append(dict_word)\n",
    "        \n",
    "        # # overwrite old possible words dictionary with updated version\n",
    "        # self.current_dictionary = new_dictionary\n",
    "        \n",
    "        \n",
    "        # # count occurrence of all characters in possible word matches\n",
    "        # full_dict_string = \"\".join(new_dictionary)\n",
    "        \n",
    "        # c = collections.Counter(full_dict_string)\n",
    "        # sorted_letter_count = c.most_common()                   \n",
    "        \n",
    "        # guess_letter = '!'\n",
    "        \n",
    "        # # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
    "        # for letter,instance_count in sorted_letter_count:\n",
    "        #     if letter not in self.guessed_letters:\n",
    "        #         guess_letter = letter\n",
    "        #         break\n",
    "            \n",
    "        # # if no word matches in training dictionary, default back to ordering of full dictionary\n",
    "        # if guess_letter == '!':\n",
    "        #     sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
    "        #     for letter,instance_count in sorted_letter_count:\n",
    "        #         if letter not in self.guessed_letters:\n",
    "        #             guess_letter = letter\n",
    "        #             break            \n",
    "        \n",
    "        # return guess_letter\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"8912423d2cd0184ccd5e951957165f\", timeout=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291 728\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes_before] = api.my_status()\n",
    "print(total_practice_runs, total_practice_successes_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiran/Desktop/DL/hangman/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 % done...\n",
      "2.0 % done...\n",
      "3.0 % done...\n",
      "4.0 % done...\n",
      "5.0 % done...\n",
      "6.0 % done...\n",
      "7.000000000000001 % done...\n",
      "8.0 % done...\n",
      "9.0 % done...\n",
      "10.0 % done...\n",
      "11.0 % done...\n",
      "12.0 % done...\n",
      "13.0 % done...\n",
      "14.000000000000002 % done...\n",
      "15.0 % done...\n",
      "16.0 % done...\n",
      "17.0 % done...\n",
      "18.0 % done...\n",
      "19.0 % done...\n",
      "20.0 % done...\n",
      "21.0 % done...\n",
      "22.0 % done...\n",
      "23.0 % done...\n",
      "24.0 % done...\n",
      "25.0 % done...\n",
      "26.0 % done...\n",
      "27.0 % done...\n",
      "28.000000000000004 % done...\n",
      "28.999999999999996 % done...\n",
      "30.0 % done...\n",
      "31.0 % done...\n",
      "32.0 % done...\n",
      "33.0 % done...\n",
      "34.0 % done...\n",
      "35.0 % done...\n",
      "36.0 % done...\n",
      "37.0 % done...\n",
      "38.0 % done...\n",
      "39.0 % done...\n",
      "40.0 % done...\n",
      "41.0 % done...\n",
      "42.0 % done...\n",
      "43.0 % done...\n",
      "44.0 % done...\n",
      "45.0 % done...\n",
      "46.0 % done...\n",
      "47.0 % done...\n",
      "48.0 % done...\n",
      "49.0 % done...\n",
      "50.0 % done...\n",
      "51.0 % done...\n",
      "52.0 % done...\n",
      "53.0 % done...\n",
      "54.0 % done...\n",
      "55.00000000000001 % done...\n",
      "56.00000000000001 % done...\n",
      "56.99999999999999 % done...\n",
      "57.99999999999999 % done...\n",
      "59.0 % done...\n",
      "60.0 % done...\n",
      "61.0 % done...\n",
      "62.0 % done...\n",
      "63.0 % done...\n",
      "64.0 % done...\n",
      "65.0 % done...\n",
      "66.0 % done...\n",
      "67.0 % done...\n",
      "68.0 % done...\n",
      "69.0 % done...\n",
      "70.0 % done...\n",
      "71.0 % done...\n",
      "72.0 % done...\n",
      "73.0 % done...\n",
      "74.0 % done...\n",
      "75.0 % done...\n",
      "76.0 % done...\n",
      "77.0 % done...\n",
      "78.0 % done...\n",
      "79.0 % done...\n",
      "80.0 % done...\n",
      "81.0 % done...\n",
      "82.0 % done...\n",
      "83.0 % done...\n",
      "84.0 % done...\n",
      "85.0 % done...\n",
      "86.0 % done...\n",
      "87.0 % done...\n",
      "88.0 % done...\n",
      "89.0 % done...\n",
      "90.0 % done...\n",
      "91.0 % done...\n",
      "92.0 % done...\n",
      "93.0 % done...\n",
      "94.0 % done...\n",
      "95.0 % done...\n",
      "96.0 % done...\n",
      "97.0 % done...\n",
      "98.0 % done...\n",
      "99.0 % done...\n",
      "100.0 % done...\n",
      "run 2391 practice games out of an allotted 100,000.\n",
      "This session's accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes_before] = api.my_status()\n",
    "no_of_runs = 100\n",
    "for i in range(no_of_runs):\n",
    "    api.start_game(practice=1,verbose=False)\n",
    "    print(f'{(i + 1)/no_of_runs*100} % done...')\n",
    "    time.sleep(0.5)\n",
    "    # [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "    # practice_success_rate = total_practice_successes / total_practice_runs\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes_after] = api.my_status()\n",
    "\n",
    "print('run %d practice games out of an allotted 100,000.' % (total_practice_runs))\n",
    "print(\"This session's accuracy:\", (total_practice_successes_after - total_practice_successes_before)/no_of_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACHIEVED AN ACCURACY OF 0.60 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 0 0 54\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status()\n",
    "print(total_practice_runs, total_recorded_runs, total_recorded_successes, total_practice_successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2391 661 345 790\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status()\n",
    "print(total_practice_runs, total_recorded_runs, total_recorded_successes, total_practice_successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  0  th game\n",
      "Playing  1  th game\n",
      "Playing  2  th game\n",
      "Playing  3  th game\n",
      "Playing  4  th game\n",
      "Playing  5  th game\n",
      "Playing  6  th game\n",
      "Playing  7  th game\n",
      "Playing  8  th game\n",
      "Playing  9  th game\n",
      "Playing  10  th game\n",
      "Playing  11  th game\n",
      "Playing  12  th game\n",
      "Playing  13  th game\n",
      "Playing  14  th game\n",
      "Playing  15  th game\n",
      "Playing  16  th game\n",
      "Playing  17  th game\n",
      "Playing  18  th game\n",
      "Playing  19  th game\n",
      "Playing  20  th game\n",
      "Playing  21  th game\n",
      "Playing  22  th game\n",
      "Playing  23  th game\n",
      "Playing  24  th game\n",
      "Playing  25  th game\n",
      "Playing  26  th game\n",
      "Playing  27  th game\n",
      "Playing  28  th game\n",
      "Playing  29  th game\n",
      "Playing  30  th game\n",
      "Playing  31  th game\n",
      "Playing  32  th game\n",
      "Playing  33  th game\n",
      "Playing  34  th game\n",
      "Playing  35  th game\n",
      "Playing  36  th game\n",
      "Playing  37  th game\n",
      "Playing  38  th game\n",
      "Playing  39  th game\n",
      "Playing  40  th game\n",
      "Playing  41  th game\n",
      "Playing  42  th game\n",
      "Playing  43  th game\n",
      "Playing  44  th game\n",
      "Playing  45  th game\n",
      "Playing  46  th game\n",
      "Playing  47  th game\n",
      "Playing  48  th game\n",
      "Playing  49  th game\n",
      "Playing  50  th game\n",
      "Playing  51  th game\n",
      "Playing  52  th game\n",
      "Playing  53  th game\n",
      "Playing  54  th game\n",
      "Playing  55  th game\n",
      "Playing  56  th game\n",
      "Playing  57  th game\n",
      "Playing  58  th game\n",
      "Playing  59  th game\n",
      "Playing  60  th game\n",
      "Playing  61  th game\n",
      "Playing  62  th game\n",
      "Playing  63  th game\n",
      "Playing  64  th game\n",
      "Playing  65  th game\n",
      "Playing  66  th game\n",
      "Playing  67  th game\n",
      "Playing  68  th game\n",
      "Playing  69  th game\n",
      "Playing  70  th game\n",
      "Playing  71  th game\n",
      "Playing  72  th game\n",
      "Playing  73  th game\n",
      "Playing  74  th game\n",
      "Playing  75  th game\n",
      "Playing  76  th game\n",
      "Playing  77  th game\n",
      "Playing  78  th game\n",
      "Playing  79  th game\n",
      "Playing  80  th game\n",
      "Playing  81  th game\n",
      "Playing  82  th game\n",
      "Playing  83  th game\n",
      "Playing  84  th game\n",
      "Playing  85  th game\n",
      "Playing  86  th game\n",
      "Playing  87  th game\n",
      "Playing  88  th game\n",
      "Playing  89  th game\n",
      "Playing  90  th game\n",
      "Playing  91  th game\n",
      "Playing  92  th game\n",
      "Playing  93  th game\n",
      "Playing  94  th game\n",
      "Playing  95  th game\n",
      "Playing  96  th game\n",
      "Playing  97  th game\n",
      "Playing  98  th game\n",
      "Playing  99  th game\n",
      "Playing  100  th game\n",
      "Playing  101  th game\n",
      "Playing  102  th game\n",
      "Playing  103  th game\n",
      "Playing  104  th game\n",
      "Playing  105  th game\n",
      "Playing  106  th game\n",
      "Playing  107  th game\n",
      "Playing  108  th game\n",
      "Playing  109  th game\n",
      "Playing  110  th game\n",
      "Playing  111  th game\n",
      "Playing  112  th game\n",
      "Playing  113  th game\n",
      "Playing  114  th game\n",
      "Playing  115  th game\n",
      "Playing  116  th game\n",
      "Playing  117  th game\n",
      "Playing  118  th game\n",
      "Playing  119  th game\n",
      "Playing  120  th game\n",
      "Playing  121  th game\n",
      "Playing  122  th game\n",
      "Playing  123  th game\n",
      "Playing  124  th game\n",
      "Playing  125  th game\n",
      "Playing  126  th game\n",
      "Playing  127  th game\n",
      "Playing  128  th game\n",
      "Playing  129  th game\n",
      "Playing  130  th game\n",
      "Playing  131  th game\n",
      "Playing  132  th game\n",
      "Playing  133  th game\n",
      "Playing  134  th game\n",
      "Playing  135  th game\n",
      "Playing  136  th game\n",
      "Playing  137  th game\n",
      "Playing  138  th game\n",
      "Playing  139  th game\n",
      "Playing  140  th game\n",
      "Playing  141  th game\n",
      "Playing  142  th game\n",
      "Playing  143  th game\n",
      "Playing  144  th game\n",
      "Playing  145  th game\n",
      "Playing  146  th game\n",
      "Playing  147  th game\n",
      "Playing  148  th game\n",
      "Playing  149  th game\n",
      "Playing  150  th game\n",
      "Playing  151  th game\n",
      "Playing  152  th game\n",
      "Playing  153  th game\n",
      "Playing  154  th game\n",
      "Playing  155  th game\n",
      "Playing  156  th game\n",
      "Playing  157  th game\n",
      "Playing  158  th game\n",
      "Playing  159  th game\n",
      "Playing  160  th game\n",
      "Playing  161  th game\n",
      "Playing  162  th game\n",
      "Playing  163  th game\n",
      "Playing  164  th game\n",
      "Playing  165  th game\n",
      "Playing  166  th game\n",
      "Playing  167  th game\n",
      "Playing  168  th game\n",
      "Playing  169  th game\n",
      "Playing  170  th game\n",
      "Playing  171  th game\n",
      "Playing  172  th game\n",
      "Playing  173  th game\n",
      "Playing  174  th game\n",
      "Playing  175  th game\n",
      "Playing  176  th game\n",
      "Playing  177  th game\n",
      "Playing  178  th game\n",
      "Playing  179  th game\n",
      "Playing  180  th game\n",
      "Playing  181  th game\n",
      "Playing  182  th game\n",
      "Playing  183  th game\n",
      "Playing  184  th game\n",
      "Playing  185  th game\n",
      "Playing  186  th game\n",
      "Playing  187  th game\n",
      "Playing  188  th game\n",
      "Playing  189  th game\n",
      "Playing  190  th game\n",
      "Playing  191  th game\n",
      "Playing  192  th game\n",
      "Playing  193  th game\n",
      "Playing  194  th game\n",
      "Playing  195  th game\n",
      "Playing  196  th game\n",
      "Playing  197  th game\n",
      "Playing  198  th game\n",
      "Playing  199  th game\n",
      "Playing  200  th game\n",
      "Playing  201  th game\n",
      "Playing  202  th game\n",
      "Playing  203  th game\n",
      "Playing  204  th game\n",
      "Playing  205  th game\n",
      "Playing  206  th game\n",
      "Playing  207  th game\n",
      "Playing  208  th game\n",
      "Playing  209  th game\n",
      "Playing  210  th game\n",
      "Playing  211  th game\n",
      "Playing  212  th game\n",
      "Playing  213  th game\n",
      "Playing  214  th game\n",
      "Playing  215  th game\n",
      "Playing  216  th game\n",
      "Playing  217  th game\n",
      "Playing  218  th game\n",
      "Playing  219  th game\n",
      "Playing  220  th game\n",
      "Playing  221  th game\n",
      "Playing  222  th game\n",
      "Playing  223  th game\n",
      "Playing  224  th game\n",
      "Playing  225  th game\n",
      "Playing  226  th game\n",
      "Playing  227  th game\n",
      "Playing  228  th game\n",
      "Playing  229  th game\n",
      "Playing  230  th game\n",
      "Playing  231  th game\n",
      "Playing  232  th game\n",
      "Playing  233  th game\n",
      "Playing  234  th game\n",
      "Playing  235  th game\n",
      "Playing  236  th game\n",
      "Playing  237  th game\n",
      "Playing  238  th game\n",
      "Playing  239  th game\n",
      "Playing  240  th game\n",
      "Playing  241  th game\n",
      "Playing  242  th game\n",
      "Playing  243  th game\n",
      "Playing  244  th game\n",
      "Playing  245  th game\n",
      "Playing  246  th game\n",
      "Playing  247  th game\n",
      "Playing  248  th game\n",
      "Playing  249  th game\n",
      "Playing  250  th game\n",
      "Playing  251  th game\n",
      "Playing  252  th game\n",
      "Playing  253  th game\n",
      "Playing  254  th game\n",
      "Playing  255  th game\n",
      "Playing  256  th game\n",
      "Playing  257  th game\n",
      "Playing  258  th game\n",
      "Playing  259  th game\n",
      "Playing  260  th game\n",
      "Playing  261  th game\n",
      "Playing  262  th game\n",
      "Playing  263  th game\n",
      "Playing  264  th game\n",
      "Playing  265  th game\n",
      "Playing  266  th game\n",
      "Playing  267  th game\n",
      "Playing  268  th game\n",
      "Playing  269  th game\n",
      "Playing  270  th game\n",
      "Playing  271  th game\n",
      "Playing  272  th game\n",
      "Playing  273  th game\n",
      "Playing  274  th game\n",
      "Playing  275  th game\n",
      "Playing  276  th game\n",
      "Playing  277  th game\n",
      "Playing  278  th game\n",
      "Playing  279  th game\n",
      "Playing  280  th game\n",
      "Playing  281  th game\n",
      "Playing  282  th game\n",
      "Playing  283  th game\n",
      "Playing  284  th game\n",
      "Playing  285  th game\n",
      "Playing  286  th game\n",
      "Playing  287  th game\n",
      "Playing  288  th game\n",
      "Playing  289  th game\n",
      "Playing  290  th game\n",
      "Playing  291  th game\n",
      "Playing  292  th game\n",
      "Playing  293  th game\n",
      "Playing  294  th game\n",
      "Playing  295  th game\n",
      "Playing  296  th game\n",
      "Playing  297  th game\n",
      "Playing  298  th game\n",
      "Playing  299  th game\n",
      "Playing  300  th game\n",
      "Playing  301  th game\n",
      "Playing  302  th game\n",
      "Playing  303  th game\n",
      "Playing  304  th game\n",
      "Playing  305  th game\n",
      "Playing  306  th game\n",
      "Playing  307  th game\n",
      "Playing  308  th game\n",
      "Playing  309  th game\n",
      "Playing  310  th game\n",
      "Playing  311  th game\n",
      "Playing  312  th game\n",
      "Playing  313  th game\n",
      "Playing  314  th game\n",
      "Playing  315  th game\n",
      "Playing  316  th game\n",
      "Playing  317  th game\n",
      "Playing  318  th game\n",
      "Playing  319  th game\n",
      "Playing  320  th game\n",
      "Playing  321  th game\n",
      "Playing  322  th game\n",
      "Playing  323  th game\n",
      "Playing  324  th game\n",
      "Playing  325  th game\n",
      "Playing  326  th game\n",
      "Playing  327  th game\n",
      "Playing  328  th game\n",
      "Playing  329  th game\n",
      "Playing  330  th game\n",
      "Playing  331  th game\n",
      "Playing  332  th game\n",
      "Playing  333  th game\n",
      "Playing  334  th game\n",
      "Playing  335  th game\n",
      "Playing  336  th game\n",
      "Playing  337  th game\n",
      "Playing  338  th game\n"
     ]
    },
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[39;00m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary\n\u001b[0;32m--> 110\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 212\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got cut after 661 runs because of network issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.532\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of new model :  0.5532544378698225\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of new model : \",  (total_recorded_successes - 345 )/ 338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COULD'NT COMPLETE 1000 RUNS BECAUSE I EXHAUSTED SOME OF THEM WITH AN OLD MODEL... STILL THE NEW MODEL HAS 0.55+ IN THE 338 RUNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data.py\n",
    "\n",
    "```python\n",
    "def permute(word, results, removed):\n",
    "    for i in word:\n",
    "        if i != '_' and i not in removed:\n",
    "            removed.append(i)\n",
    "            stripped_word = word.copy()\n",
    "            for j in range(len(stripped_word)):\n",
    "                if stripped_word[j] == i:\n",
    "                    stripped_word[j] = '_'\n",
    "            results.append(stripped_word)\n",
    "            permute(stripped_word, results, removed.copy())\n",
    "    return results\n",
    "\n",
    "\n",
    "filename = \"words_250000_train.txt\"\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "print(f\"{len(words)} words\")\n",
    "\n",
    "output_file = \"small_strip_250000.txt\"\n",
    "batch_size = 1000\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for batch in range(len(words) // batch_size):\n",
    "        for word in words[batch_size * batch : (batch + 1) * batch_size]:\n",
    "            word_as_list = list(word)\n",
    "            strip_list = permute(word_as_list, [], [])\n",
    "            for strip_word_as_list in strip_list:\n",
    "                strip_word = ''.join(strip_word_as_list)\n",
    "                f.write(f\"{strip_word} {word}\\n\")\n",
    "        if (batch % 100 == 0) :\n",
    "            print(f\"{batch + 1} batches complete of out {len(words) // batch_size}\")\n",
    "\n",
    "```\n",
    "# load_data.py\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class WordCompletionDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.line_offsets = []\n",
    "\n",
    "        # Precompute line start offsets for fast random access\n",
    "        with open(filepath, 'rb') as f:\n",
    "            offset = 0\n",
    "            for line in f:\n",
    "                self.line_offsets.append(offset)\n",
    "                offset += len(line)\n",
    "\n",
    "    def input_encode(self, word):\n",
    "        matrix = np.zeros((27, len(word)))\n",
    "        for i, ch in enumerate(word):\n",
    "            if ch != '_':\n",
    "                matrix[ord(ch) - ord('a')][i] = 1\n",
    "            else:\n",
    "                matrix[26][i] = 1\n",
    "        return matrix\n",
    "\n",
    "    def output_encode(self, word):\n",
    "        matrix = np.zeros((27, len(word)))\n",
    "        for i, ch in enumerate(word):\n",
    "            matrix[ord(ch) - ord('a')][i] = 1\n",
    "        return matrix\n",
    "\n",
    "    def input_decode(self, matrix):\n",
    "        word = []\n",
    "        matrix = np.array(matrix)\n",
    "        for i in range(matrix.shape[1]):\n",
    "            idx = np.argmax(matrix[:, i])\n",
    "            word.append('_' if idx == 26 else chr(idx + ord('a')))\n",
    "        return ''.join(word)\n",
    "\n",
    "    def output_decode(self, matrix):\n",
    "        word = []\n",
    "        matrix = np.array(matrix)\n",
    "        for i in range(matrix.shape[1]):\n",
    "            idx = np.argmax(matrix[:, i])\n",
    "            word.append(chr(idx + ord('a')))\n",
    "        return ''.join(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        offset = self.line_offsets[idx]\n",
    "        with open(self.filepath, 'r') as f:\n",
    "            f.seek(offset)\n",
    "            line = f.readline().strip()\n",
    "            input_word, output_word = line.split()\n",
    "\n",
    "        input_ids = self.input_encode(input_word)\n",
    "        output_ids = self.output_encode(output_word)\n",
    "        return torch.tensor(input_ids, dtype=torch.float32), torch.tensor(output_ids, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    inputs = [i.permute(1, 0) for i in inputs]\n",
    "    outputs = [o.permute(1, 0) for o in outputs]\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True)  # (batch, seq_len, 27)\n",
    "    padded_outputs = pad_sequence(outputs, batch_first=True)  # (batch, seq_len, 27)\n",
    "\n",
    "    key_padding_mask = (padded_inputs.sum(-1) != 0)  # (batch, seq_len)\n",
    "    return padded_inputs, padded_outputs, key_padding_mask\n",
    "\n",
    "def return_dataloader():\n",
    "    dataset = WordCompletionDataset(\"small_strip_250000.txt\")\n",
    "    dataloader = DataLoader(dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "    print(\"Dataset Loaded Successfully\")\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    dataset = WordCompletionDataset(\"small_strip_250000.txt\")\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    for inputs, outputs, _ in dataloader:\n",
    "        print(f\"Number of batches: {len(dataloader)}\")\n",
    "        print(f\"Inputs batch shape: {inputs.shape}\")   # (batch_size, seq_len, 27)\n",
    "        print(f\"Outputs batch shape: {outputs.shape}\") # (batch_size, seq_len, 27)\n",
    "\n",
    "        # Visualize one word pair\n",
    "        idx = 0  # first sample in batch\n",
    "        input_matrix = inputs[idx].permute(1, 0).numpy()   # (27, seq_len)\n",
    "        output_matrix = outputs[idx].permute(1, 0).numpy() # (27, seq_len)\n",
    "\n",
    "        input_word = dataset.input_decode(input_matrix)\n",
    "        output_word = dataset.output_decode(output_matrix)\n",
    "\n",
    "        print(f\"\\nSample {idx + 1}\")\n",
    "        print(f\"Masked Input Word:  {input_word}\")\n",
    "        print(f\"Ground Truth Word:  {output_word}\")\n",
    "        \n",
    "        break\n",
    "```\n",
    "\n",
    "# transformer_model.py\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from load_data import return_dataloader\n",
    "from load_data import WordCompletionDataset\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=27, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, max_len=100):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, input_size)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.log_softmax(x, dim = -1)\n",
    "        \n",
    "        return x  # (batch, seq_len, 27)\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "        batch = 0\n",
    "\n",
    "        for inputs, outputs, key_padding_mask in dataloader:\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "\n",
    "            # Compute mask for padding positions\n",
    "            src_key_padding_mask = (inputs == 0).all(dim=-1).bool()  # (batch, seq_len)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "\n",
    "            # # Focus loss only on masked positions\n",
    "            # mask_token_index = 26\n",
    "            # masked_positions = (inputs[:, :, mask_token_index] == 1).unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "\n",
    "            # loss = -torch.sum(masked_positions * outputs * predictions)\n",
    "            # loss = loss / masked_positions.sum().clamp(min=1)\n",
    "            \n",
    "            loss = soft_cross_entropy_with_mask(predictions, outputs, pad_mask=src_key_padding_mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch += 1\n",
    "            if (batch % 100 == 0) :\n",
    "                print(f'Epoch: {epoch+1} - {batch} batch done of total {total_batches} batches...({batch/total_batches * 100 :.2f}%)')\n",
    "            if (batch % 10000 == 0):\n",
    "                torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "                print(\"Model saved\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "    \n",
    "def soft_cross_entropy_with_mask(predictions, targets, pad_mask):\n",
    "    \"\"\"\n",
    "    predictions: (B, T, C) - log probabilities (log_softmax output)\n",
    "    targets:     (B, T, C) - target probability distributions\n",
    "    pad_mask:    (B, T)    - bool, True where padding\n",
    "    \"\"\"\n",
    "    non_pad_mask = ~pad_mask  # True where not padding\n",
    "    loss_per_pos = -torch.sum(targets * predictions, dim=-1)  # (B, T)\n",
    "    masked_loss = loss_per_pos * non_pad_mask.float()         # zero out pad positions\n",
    "    return masked_loss.sum() / non_pad_mask.sum().clamp(min=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset, dataloader = return_dataloader()\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel().to(device)\n",
    "    print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    train(model, dataloader, optimizer, 1, device)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# predict.py\n",
    "\n",
    "\n",
    "```python\n",
    "from transformer_model import TransformerModel\n",
    "import torch\n",
    "from load_data import WordCompletionDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_model import TransformerModel\n",
    "from load_data import WordCompletionDataset\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformer_model import TransformerModel\n",
    "from load_data import WordCompletionDataset\n",
    "\n",
    "def make_predictions(word, max_len=64):\n",
    "    dataset = WordCompletionDataset(\"small_strip_25000.txt\")\n",
    "    encoded = dataset.input_encode(word)  # (27, seq_len)\n",
    "    seq_len = encoded.shape[1]\n",
    "\n",
    "    # Pad to max_len\n",
    "    if seq_len < max_len:\n",
    "        pad_width = ((0, 0), (0, max_len - seq_len))\n",
    "        encoded = np.pad(encoded, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    input_tensor = torch.tensor(encoded, dtype=torch.float32).T.unsqueeze(0)  # (1, max_len, 27)\n",
    "\n",
    "    # Create src_key_padding_mask: True where position is padding\n",
    "    pad_mask = (input_tensor.sum(-1) == 0)  # (1, max_len)\n",
    "\n",
    "    model = TransformerModel()\n",
    "    model.load_state_dict(torch.load(\"trained_model.pth\", map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, src_key_padding_mask=pad_mask)  # (1, max_len, 27)\n",
    "        output = output.squeeze(0).T  # (27, max_len)\n",
    "\n",
    "    # Extract masked positions (channel 26 indicates masked token)\n",
    "    mask_indices = np.where(encoded[26] == 1)[0]\n",
    "    if len(mask_indices) == 0:\n",
    "        print(\"No masked characters found in input.\")\n",
    "        return []\n",
    "\n",
    "    # Average logits across masked positions\n",
    "    masked_preds = output[:, mask_indices].mean(dim=1)  # (27,)\n",
    "    masked_preds[26] = -float('inf')  # prevent predicting [MASK]\n",
    "\n",
    "    sorted_indices = torch.argsort(masked_preds, descending=True).tolist()\n",
    "    guesses = [chr(i + ord('a')) for i in sorted_indices]\n",
    "\n",
    "    return guesses\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(make_predictions('hi_an'))\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
